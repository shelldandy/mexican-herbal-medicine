# QLoRA Training Configuration for DASD-4B-Thinking on Herbolaria Dataset
#
# Hardware Requirements:
# - Minimum: RTX 4090 (24GB) or Cloud A10G
# - Recommended: Cloud A100-40GB for faster training
#
# Usage:
#   python training/train.py --config training/configs/qlora_config.yaml

# Model configuration
model:
  name: "Alibaba-Apsara/DASD-4B-Thinking"
  # Alternative: "Qwen/Qwen3-4B" for base model without thinking
  trust_remote_code: true
  torch_dtype: "bfloat16"

# Quantization for QLoRA (4-bit)
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# LoRA configuration
lora:
  r: 64
  lora_alpha: 128
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  # Target all important layers for better adaptation
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Dataset configuration
dataset:
  path: "data/herbolaria_training"
  train_split: "train"
  val_split: "validation"
  # Chat template for the model
  chat_template: "qwen"
  max_seq_length: 2048
  # Packing for efficiency
  packing: false

# Training arguments
training:
  output_dir: "models/herbolaria-dasd-4b-lora"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch size = 16
  gradient_checkpointing: true

  # Learning rate schedule
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.01

  # Optimizer
  optim: "paged_adamw_8bit"

  # Precision
  bf16: true
  tf32: true

  # Logging
  logging_steps: 10
  logging_first_step: true

  # Evaluation
  eval_strategy: "steps"
  eval_steps: 100

  # Checkpointing
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3

  # Best model selection
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

  # Misc
  seed: 42
  dataloader_num_workers: 4
  remove_unused_columns: false

  # Reporting
  report_to: "wandb"  # or "tensorboard", "none"

# Weights & Biases configuration
wandb:
  project: "herbolaria-finetune"
  name: "dasd-4b-qlora"
  tags:
    - "qlora"
    - "herbolaria"
    - "mexican-medicine"

# SFTTrainer specific settings
sft:
  max_seq_length: 2048
  packing: false
  dataset_text_field: null  # Use messages format

# Memory optimization
memory:
  # Enable for lower VRAM usage
  use_flash_attention_2: true
  # Offload optimizer states to CPU if needed
  optimizer_offload: false
